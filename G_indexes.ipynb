{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf7da9-0315-47bb-bfc3-b12ed1f022ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File /storage/shared/oceanparcels/input_data/MOi/psy4v3r1/psy4v3r1-daily_U_2019-09-05.nc could not be decoded properly by xarray (version 0.19.0).\n",
      "         It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "/nethome/6142060/parcels/parcels/field.py:248: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  timeslices = np.array(timeslices)\n",
      "/nethome/6142060/parcels/parcels/field.py:250: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  dataFiles = np.concatenate(np.array(dataFiles))\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Created on Mon Apr 11 16:39:06 2022\n",
    "\n",
    "# @author: ruijsch\n",
    "# \"\"\"\n",
    "\n",
    "from parcels import FieldSet, ParticleSet, Variable, JITParticle, AdvectionRK4, plotTrajectoriesFile, ErrorCode, ParticleFile, ScipyParticle\n",
    "from parcels import UnitConverter, Field, Variable\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta, datetime\n",
    "from datetime import timedelta as delta\n",
    "from operator import attrgetter\n",
    "from glob import glob\n",
    "import copy\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "\n",
    "#%%\n",
    "#Load the data from Lorenz and define the indices\n",
    " \n",
    "data_path_ocean = '/storage/shared/oceanparcels/input_data/MOi/'\n",
    "# Load only a few time steps of the model output, to speed up this test simulation\n",
    "ufiles = sorted(glob(data_path_ocean+'psy4v3r1/psy4v3r1-daily_U_*.nc'))[-1345:-970]\n",
    "vfiles = [f.replace('_U_', '_V_') for f in ufiles]\n",
    "wfiles = [f.replace('_U_', '_W_') for f in ufiles][:4]\n",
    "mesh_mask = data_path_ocean + 'domain_ORCA0083-N006/coordinates.nc'\n",
    "\n",
    "filenames = {'U': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': ufiles},\n",
    "             'V': {'lon': mesh_mask, 'lat': mesh_mask, 'depth': wfiles[0], 'data': vfiles},\n",
    "\n",
    "             }\n",
    "variables = {'U': 'vozocrtx', 'V': 'vomecrty'}\n",
    "dimensions = {'U': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'},\n",
    "              'V': {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw', 'time': 'time_counter'}}\n",
    "\n",
    "# Madagascar indices\n",
    "indices = {'lon': np.arange(3500,4321).tolist(),\n",
    "            'lat': np.arange(900, 1600).tolist()}\n",
    "\n",
    "#%%\n",
    "\n",
    "data_path_wind = '/storage/shared/oceanparcels/output_data/data_Mikael/ERA5/wind/'\n",
    "# Load only a few time steps of the model output, to speed up this test simulation\n",
    "wind_files = sorted(glob(data_path_wind+'ERA5_global_wind_monthly_*.nc'))[288:]\n",
    "\n",
    "# the mesh mask comes from the ocean data path\n",
    "mesh_mask = data_path_ocean + 'domain_ORCA0083-N006/coordinates.nc'\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "data_path_bathymetry = '/storage/shared/oceanparcels/input_data/MOi/'\n",
    "\n",
    "maskfiles = data_path_bathymetry+'domain_ORCA0083-N006/bathymetry_ORCA12_V3.3.nc'\n",
    "\n",
    "filenames_mask = {'mask': {'lon': maskfiles, 'lat': maskfiles, 'data': maskfiles}}\n",
    "\n",
    "variables_mask = {'mask': 'mask'}\n",
    "dimensions_mask = {'mask': {'lon': 'nav_lon', 'lat': 'nav_lat'}}\n",
    "\n",
    "# Madagascar indices\n",
    "indices_mask = {'lon': np.arange(3500,4321).tolist(),\n",
    "            'lat': np.arange(900, 1600).tolist()}\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "\n",
    "coastfiles = 'distance_to_coast_22_04_22.nc'\n",
    "\n",
    "filenames_coast = {'distance': {'lon': coastfiles, 'lat': coastfiles, 'data': coastfiles}}\n",
    "\n",
    "variables_coast = {'distance': 'distance'}\n",
    "dimensions_coast = {'distance': {'lon': 'lon', 'lat': 'lat'}}\n",
    "\n",
    "# Madagascar indices -> it already has the right indices\n",
    "# indices_coast = {'lon': np.arange(3500,4321).tolist(),\n",
    "#             'lat': np.arange(900, 1600).tolist()}\n",
    "\n",
    "\n",
    "boxfiles = 'box.nc'\n",
    "\n",
    "filenames_coast = {'distance': {'lon': coastfiles, 'lat': coastfiles, 'data': coastfiles}}\n",
    "\n",
    "variables_coast = {'distance': 'distance'}\n",
    "dimensions_coast = {'distance': {'lon': 'lon', 'lat': 'lat'}}\n",
    "\n",
    "\n",
    "#%%\n",
    "#Make the fieldsets for the currents, wind, land/sea mask and the coast\n",
    "\n",
    "# note the indices argument and the 'allow time extrapolation=True' to speed up simulation\n",
    "fset_currents = FieldSet.from_nemo(filenames, variables, dimensions, indices=indices) #, allow_time_extrapolation=True)\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "withwind = 0.03  # wind factor of 3%\n",
    "if withwind:\n",
    "    windfiles = {'U': wind_files,\n",
    "                 'V': wind_files}\n",
    "    winddimensions = {'lon': 'lon', \n",
    "                      'lat': 'lat', \n",
    "                      'time': 'time'}\n",
    "    windvariables = {'U': 'u10', 'V': 'v10'}\n",
    "    # Madagascar indices\n",
    "    indices_wind = {'lon': np.arange(750, 1000).tolist(),\n",
    "               'lat': np.arange(200, 400).tolist()}\n",
    "    fset_wind = FieldSet.from_nemo(windfiles, windvariables, winddimensions,indices=indices_wind)#, allow_time_extrapolation=True)\n",
    "    fset_wind.U.set_scaling_factor(withwind)\n",
    "    fset_wind.V.set_scaling_factor(withwind)\n",
    "    \n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "fset_mask = FieldSet.from_nemo(filenames_mask, variables_mask, dimensions_mask, indices=indices_mask)\n",
    "fset_coast = FieldSet.from_nemo(filenames_coast, variables_coast, dimensions_coast)\n",
    "\n",
    "#%%\n",
    "\n",
    "if withwind:\n",
    "    fieldset = FieldSet(U=fset_currents.U + fset_wind.U, V=fset_currents.V + fset_wind.V)\n",
    "else:\n",
    "    fieldset = FieldSet(U=fset_currents.U, V=fset_currents.V)\n",
    "    \n",
    "fieldset.add_field(fset_mask.mask)\n",
    "fieldset.add_field(fset_coast.distance)\n",
    "\n",
    "\n",
    "\n",
    "fieldset.mask.data[:,:,600:] = 0\n",
    "fieldset.mask.data[:,:,:230] = 0\n",
    "fieldset.mask.data[:,:200,:] = 0\n",
    "fieldset.mask.data[:,570:,:] = 0\n",
    "\n",
    "lonss = np.load('lons_001_present.npy')\n",
    "latss = np.load('lats_001_present.npy')\n",
    "depths = np.ones(np.shape(lonss)[0])*0.5\n",
    "\n",
    "output_files = sorted(glob('Present_day_2019_windage_3/2022_11_02_present_day_2019_whole_grid_001_windage_3_day*.nc'))\n",
    "\n",
    "\n",
    "start_lon_lat = np.load('2022_11_07_start_lon_lat_present_2019_3_windage.npy')\n",
    "end_lon_lat = np.load('2022_11_07_end_lon_lat_present_2019_3_windage.npy')\n",
    "\n",
    "X,Y = np.meshgrid(fieldset.mask.lon, fieldset.mask.lat)\n",
    "p = fieldset.mask.data == 1\n",
    "\n",
    "box_lon = X[p[0,:,:]]\n",
    "box_lat = Y[p[0,:,:]]\n",
    "\n",
    "coastal = np.load('2022_10_24_coastal_nodes_present.npy')\n",
    "\n",
    "\n",
    "X__ = X[::4,::4]\n",
    "Y__ = Y[::4,::4]\n",
    "p__ = p[:,::4,::4]\n",
    "\n",
    "box_lon__ = X__[p__[0,:,:]]\n",
    "box_lat__ = Y__[p__[0,:,:]]\n",
    "\n",
    "coastal_ = np.load('2022_10_24_coastal_nodes_present_low.npy')\n",
    "\n",
    "# box_lon_lat__ = np.zeros((len(box_lon__),2))\n",
    "\n",
    "# for i in range(len(box_lon__)):\n",
    "#     box_lon_lat__[i,0] = box_lon__[i]\n",
    "#     box_lon_lat__[i,1] = box_lat__[i]\n",
    "    \n",
    "    \n",
    "# locations = box_lon_lat__\n",
    "\n",
    "# np.save('2022_10_24_locations_present.npy',(locations))\n",
    "\n",
    "locations = np.load('2022_10_24_locations_present.npy')\n",
    "\n",
    "index_start = np.zeros(len(start_lon_lat))\n",
    "index_end = np.zeros((len(output_files),len(start_lon_lat)))\n",
    "\n",
    "for j in range(len(start_lon_lat)):\n",
    "#for j in range(0,20):\n",
    "    latvalue = start_lon_lat[j,:][1]\n",
    "    lat_min = (np.abs(locations[:,1] - latvalue))\n",
    "    idxlat_start = np.where(lat_min == lat_min.min())\n",
    "\n",
    "    lonvalue = start_lon_lat[j,:][0]\n",
    "    lon_min = (np.abs(locations[idxlat_start][:,0] - lonvalue))\n",
    "    idxlon_start = np.where(lon_min == lon_min.min()) \n",
    "    idxlon_start = idxlon_start + idxlat_start[0][0]\n",
    "\n",
    "    index_ = np.intersect1d(np.asarray(idxlon_start), np.asarray(idxlat_start))\n",
    "    index__ = index_[0]\n",
    "    #print(index_1)\n",
    "\n",
    "    if len(index_) != 0:\n",
    "        index_start[j] = index__\n",
    "    else: \n",
    "        #index[i,j] = 10e10\n",
    "        latvalue = idxlat_start[0][0]\n",
    "        idx_min = (np.abs(idxlon - latvalue))\n",
    "        a = np.argwhere(idx_min == idx_min.min())[0,1]\n",
    "        index_start[j] = idxlon[0][a]\n",
    "        \n",
    "        \n",
    "for i in range(len(output_files)):\n",
    "    print(i)\n",
    "    for j in range(len(start_lon_lat)):\n",
    "        latvalue = end_lon_lat[i,j][1]\n",
    "        lat_min = (np.abs(locations[:,1] - latvalue))\n",
    "        idxlat_end = np.where(lat_min == lat_min.min())\n",
    "\n",
    "        lonvalue = end_lon_lat[i,j][0]\n",
    "        lon_min = (np.abs(locations[idxlat_end][:,0] - lonvalue))\n",
    "        idxlon_end = np.where(lon_min == lon_min.min()) \n",
    "        idxlon_end = idxlon_end + idxlat_end[0][0]\n",
    "\n",
    "        index_ = np.intersect1d(np.asarray(idxlon_end), np.asarray(idxlat_end))\n",
    "        #print(index_1)\n",
    "\n",
    "        if len(index_) != 0:\n",
    "            index_end[i,j] = index_[0]\n",
    "        else: \n",
    "            #index[i,j] = 10e10\n",
    "            latvalue = idxlat[0][0]\n",
    "            idx_min = (np.abs(idxlon - latvalue))\n",
    "            a = np.argwhere(idx_min == idx_min.min())[0,1]\n",
    "            index_end[i,j] = idxlon[0][a]\n",
    "            \n",
    "np.save('2022_11_07_index_start_present_2019_3_windage_001.npy',(index_start))\n",
    "np.save('2022_11_07_index_end_present_2019_3_windage_001.npy',(index_end))\n",
    "\n",
    "\n",
    "index_start = np.load('2022_11_07_index_start_present_2019_3_windage_001.npy')\n",
    "index_end = np.load('2022_11_07_index_end_present_2019_3_windage_001.npy')\n",
    "\n",
    "\n",
    "#output_files = sorted(glob('2022_10_17_present_day_run/2022_10_17_present_day_whole_grid_001_windage_0_day*.nc'))\n",
    "\n",
    "# G = np.zeros((len(output_files),len(locations),len(locations)))\n",
    "# start = 0\n",
    "# for year in range(len(output_files)):\n",
    "#     for i,j in zip(index_end[year+start],index_start):\n",
    "#         G[year,int(i),int(j)] = G[year,int(i),int(j)] + 1\n",
    "        \n",
    "# # afr_unique = np.load(\"2022_10_25_afr_unique_present.npy\")\n",
    "# # mad_unique = np.load(\"2022_10_25_mad_unique_present.npy\")\n",
    "# # for year in range(len(output_files)):\n",
    "# #     print(year)\n",
    "# # #     for mad in mad_unique:\n",
    "# # #         G[year,:,int(mad)] = G[0,:,int(mad)]*0 #from node 1221 to all nodes\n",
    "        \n",
    "# np.save('2022_11_07_G_present_windage3.npy',(G)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a838851e-bdce-4cbe-8587-3469637aa558",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "for year in range(len(output_files)):\n",
    "    print(year)\n",
    "    for mad in mad_unique:\n",
    "        G[year,:,int(mad)] = G[0,:,int(mad)]*0 #from node 1221 to all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a9089d-3ec6-401c-b05c-2c09c1c6f058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "for year in range(len(output_files)):\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4052e989-5866-411a-89ae-a68cb58fcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.zeros((len(output_files),len(locations),len(locations)))\n",
    "afr_unique = np.load(\"2022_10_25_afr_unique_present.npy\")\n",
    "mad_unique = np.load(\"2022_10_25_mad_unique_present.npy\")\n",
    "for year in range(len(output_files)):\n",
    "    for mad in mad_unique:\n",
    "        G[year,:,int(mad)] = G[0,:,int(mad)]*0 #from node 1221 to all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a489ff7-1df8-4f2f-90f2-7263a27aa254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
